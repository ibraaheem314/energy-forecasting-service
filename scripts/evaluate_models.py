"""Script d'√©valuation des mod√®les de pr√©vision √©nerg√©tique."""
import asyncio
import sys
import json
from pathlib import Path
from datetime import datetime, timedelta
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score
import joblib

# Ajouter le chemin du projet
sys.path.append(str(Path(__file__).parent.parent))

from app.services.loader import DataLoader
from app.services.features import FeatureService
from app.services.models import ModelService

async def main():
    """Fonction principale d'√©valuation des mod√®les."""
    print("üîç D√©marrage de l'√©valuation des mod√®les")
    
    # V√©rifier si des mod√®les existent
    models_dir = Path("models")
    if not models_dir.exists():
        print("‚ùå Aucun dossier 'models' trouv√©. Ex√©cutez d'abord train_models.py")
        return
    
    model_files = list(models_dir.glob("*.joblib"))
    if not model_files:
        print("‚ùå Aucun mod√®le trouv√©. Ex√©cutez d'abord train_models.py")
        return
    
    print(f"üìä {len(model_files)} mod√®les trouv√©s")
    
    # Initialiser les services
    data_loader = DataLoader()
    feature_service = FeatureService()
    model_service = ModelService()
    
    # P√©riode d'√©valuation (diff√©rente de l'entra√Ænement)
    end_date = datetime.now()
    start_date = end_date - timedelta(days=14)  # 2 semaines pour √©valuation
    location = "region_1"
    
    print(f"üìÖ P√©riode d'√©valuation: {start_date.date()} √† {end_date.date()}")
    
    try:
        # Pr√©parer les donn√©es d'√©valuation
        print("\nüìä Pr√©paration des donn√©es d'√©valuation...")
        features_df = await feature_service.prepare_features(
            start_time=start_date,
            end_time=end_date,
            location=location,
            include_weather=True,
            include_external=False
        )
        
        X, y = model_service._prepare_training_data(features_df)
        print(f"‚úÖ Donn√©es d'√©valuation: {len(X)} √©chantillons")
        
        # √âvaluer chaque mod√®le
        evaluation_results = {}
        predictions = {}
        
        for model_file in model_files:
            model_name = model_file.stem
            print(f"\nü§ñ √âvaluation du mod√®le: {model_name}")
            
            try:
                # Charger le mod√®le
                model_data = joblib.load(model_file)
                if isinstance(model_data, dict):
                    model = model_data['model']
                    metadata = model_data.get('metadata', {})
                else:
                    model = model_data
                    metadata = {}
                
                # V√©rifier la compatibilit√© des features
                if 'feature_names' in metadata:
                    expected_features = metadata['feature_names']
                    missing_features = set(expected_features) - set(X.columns)
                    if missing_features:
                        print(f"‚ö†Ô∏è  Features manquantes: {missing_features}")
                        # Ajouter les features manquantes avec des valeurs par d√©faut
                        for feature in missing_features:
                            X[feature] = 0
                    
                    # R√©organiser les colonnes dans le bon ordre
                    X_ordered = X.reindex(columns=expected_features, fill_value=0)
                else:
                    X_ordered = X
                
                # Pr√©dictions
                y_pred = model.predict(X_ordered)
                
                # M√©triques
                metrics = {
                    'mae': mean_absolute_error(y, y_pred),
                    'mse': mean_squared_error(y, y_pred),
                    'rmse': np.sqrt(mean_squared_error(y, y_pred)),
                    'r2': r2_score(y, y_pred),
                    'mape': np.mean(np.abs((y - y_pred) / y)) * 100
                }
                
                evaluation_results[model_name] = {
                    'metrics': metrics,
                    'model_type': metadata.get('type', 'unknown'),
                    'trained_at': metadata.get('trained_at', 'unknown')
                }
                
                predictions[model_name] = {
                    'actual': y.values,
                    'predicted': y_pred,
                    'timestamps': features_df['timestamp'].values
                }
                
                print(f"‚úÖ M√©triques:")
                print(f"   - MAE: {metrics['mae']:.2f}")
                print(f"   - RMSE: {metrics['rmse']:.2f}")
                print(f"   - R¬≤: {metrics['r2']:.4f}")
                print(f"   - MAPE: {metrics['mape']:.2f}%")
                
            except Exception as e:
                print(f"‚ùå Erreur avec le mod√®le {model_name}: {e}")
                continue
        
        if not evaluation_results:
            print("‚ùå Aucun mod√®le n'a pu √™tre √©valu√©")
            return
        
        # Comparaison des mod√®les
        print("\nüìä Comparaison des mod√®les:")
        print("-" * 80)
        print(f"{'Mod√®le':<30} {'Type':<15} {'MAE':<10} {'RMSE':<10} {'R¬≤':<10} {'MAPE':<10}")
        print("-" * 80)
        
        for model_name, result in evaluation_results.items():
            metrics = result['metrics']
            model_type = result['model_type']
            print(f"{model_name:<30} {model_type:<15} {metrics['mae']:<10.2f} "
                  f"{metrics['rmse']:<10.2f} {metrics['r2']:<10.4f} {metrics['mape']:<10.2f}")
        
        # Meilleur mod√®le
        best_model = min(evaluation_results.items(), key=lambda x: x[1]['metrics']['mae'])
        print(f"\nüèÜ Meilleur mod√®le: {best_model[0]} (MAE: {best_model[1]['metrics']['mae']:.2f})")
        
        # G√©n√©rer des graphiques
        print("\nüìà G√©n√©ration des graphiques...")
        create_evaluation_plots(predictions, evaluation_results)
        
        # Sauvegarder les r√©sultats
        evaluation_summary = {
            'evaluation_date': datetime.now().isoformat(),
            'evaluation_period': {
                'start': start_date.isoformat(),
                'end': end_date.isoformat()
            },
            'location': location,
            'models_evaluated': len(evaluation_results),
            'results': evaluation_results,
            'best_model': {
                'name': best_model[0],
                'metrics': best_model[1]['metrics']
            }
        }
        
        with open("models/evaluation_summary.json", "w") as f:
            json.dump(evaluation_summary, f, indent=2, default=str)
        
        print(f"üìÑ R√©sultats sauvegard√©s: models/evaluation_summary.json")
        print("üéâ √âvaluation termin√©e avec succ√®s!")
        
    except Exception as e:
        print(f"‚ùå Erreur lors de l'√©valuation: {e}")
        raise

def create_evaluation_plots(predictions, evaluation_results):
    """Cr√©er des graphiques d'√©valuation."""
    if not predictions:
        return
    
    # Configuration matplotlib
    plt.style.use('default')
    fig, axes = plt.subplots(2, 2, figsize=(15, 10))
    fig.suptitle('√âvaluation des Mod√®les de Pr√©vision √ânerg√©tique', fontsize=16)
    
    # 1. Comparaison des m√©triques
    ax1 = axes[0, 0]
    model_names = list(evaluation_results.keys())
    mae_values = [evaluation_results[name]['metrics']['mae'] for name in model_names]
    r2_values = [evaluation_results[name]['metrics']['r2'] for name in model_names]
    
    x = np.arange(len(model_names))
    width = 0.35
    
    ax1_twin = ax1.twinx()
    bars1 = ax1.bar(x - width/2, mae_values, width, label='MAE', alpha=0.7, color='red')
    bars2 = ax1_twin.bar(x + width/2, r2_values, width, label='R¬≤', alpha=0.7, color='blue')
    
    ax1.set_xlabel('Mod√®les')
    ax1.set_ylabel('MAE', color='red')
    ax1_twin.set_ylabel('R¬≤', color='blue')
    ax1.set_title('Comparaison des M√©triques')
    ax1.set_xticks(x)
    ax1.set_xticklabels([name.split('_')[0] for name in model_names], rotation=45)
    
    # 2. Pr√©dictions vs R√©el (meilleur mod√®le)
    ax2 = axes[0, 1]
    best_model_name = min(evaluation_results.items(), key=lambda x: x[1]['metrics']['mae'])[0]
    best_pred = predictions[best_model_name]
    
    ax2.scatter(best_pred['actual'], best_pred['predicted'], alpha=0.6)
    ax2.plot([best_pred['actual'].min(), best_pred['actual'].max()], 
             [best_pred['actual'].min(), best_pred['actual'].max()], 'r--', lw=2)
    ax2.set_xlabel('Valeurs R√©elles')
    ax2.set_ylabel('Pr√©dictions')
    ax2.set_title(f'Pr√©dictions vs R√©el - {best_model_name.split("_")[0]}')
    
    # 3. S√©rie temporelle (meilleur mod√®le)
    ax3 = axes[1, 0]
    timestamps = pd.to_datetime(best_pred['timestamps'])
    ax3.plot(timestamps, best_pred['actual'], label='R√©el', linewidth=2)
    ax3.plot(timestamps, best_pred['predicted'], label='Pr√©dit', linewidth=2, alpha=0.8)
    ax3.set_xlabel('Temps')
    ax3.set_ylabel('Consommation √ânerg√©tique')
    ax3.set_title('S√©rie Temporelle - R√©el vs Pr√©dit')
    ax3.legend()
    ax3.tick_params(axis='x', rotation=45)
    
    # 4. Distribution des erreurs (meilleur mod√®le)
    ax4 = axes[1, 1]
    errors = best_pred['actual'] - best_pred['predicted']
    ax4.hist(errors, bins=30, alpha=0.7, edgecolor='black')
    ax4.axvline(x=0, color='red', linestyle='--', linewidth=2)
    ax4.set_xlabel('Erreurs (R√©el - Pr√©dit)')
    ax4.set_ylabel('Fr√©quence')
    ax4.set_title('Distribution des Erreurs')
    
    plt.tight_layout()
    plt.savefig('models/evaluation_plots.png', dpi=300, bbox_inches='tight')
    plt.close()
    
    print("üìä Graphiques sauvegard√©s: models/evaluation_plots.png")

if __name__ == "__main__":
    asyncio.run(main())
