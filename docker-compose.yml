version: '3.8'

services:
  # API Service
  api:
    build:
      context: .
      dockerfile: Dockerfile
      target: development
    ports:
      - "8000:8000"
    environment:
      - DATABASE_URL=postgresql://postgres:postgres@db:5432/energy_forecasting
      - MLFLOW_TRACKING_URI=http://mlflow:5000
      - REDIS_URL=redis://redis:6379/0
    volumes:
      - ./app:/app/app
      - ./jobs:/app/jobs
      - ./data:/app/data
      - ./mlruns:/app/mlruns
    depends_on:
      - db
      - redis
      - mlflow
    restart: unless-stopped
    networks:
      - energy-net
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:8000/health"]
      interval: 30s
      timeout: 10s
      retries: 3

  # Dashboard Service
  dashboard:
    build:
      context: .
      dockerfile: Dockerfile
      target: development
    ports:
      - "8501:8501"
    environment:
      - API_BASE_URL=http://api:8000
    volumes:
      - ./dashboard:/app/dashboard
      - ./data:/app/data
    depends_on:
      - api
    restart: unless-stopped
    networks:
      - energy-net
    command: [".venv/bin/streamlit", "run", "dashboard/app.py", "--server.port=8501", "--server.address=0.0.0.0"]

  # Database
  db:
    image: postgres:15-alpine
    environment:
      - POSTGRES_DB=energy_forecasting
      - POSTGRES_USER=postgres
      - POSTGRES_PASSWORD=postgres
    volumes:
      - postgres_data:/var/lib/postgresql/data
      - ./scripts/init_db.sql:/docker-entrypoint-initdb.d/init_db.sql
    ports:
      - "5432:5432"
    restart: unless-stopped
    networks:
      - energy-net
    healthcheck:
      test: ["CMD-SHELL", "pg_isready -U postgres"]
      interval: 5s
      timeout: 5s
      retries: 5

  # Redis Cache
  redis:
    image: redis:7-alpine
    ports:
      - "6379:6379"
    volumes:
      - redis_data:/data
    restart: unless-stopped
    networks:
      - energy-net
    healthcheck:
      test: ["CMD", "redis-cli", "ping"]
      interval: 5s
      timeout: 3s
      retries: 5

  # MLflow Tracking Server
  mlflow:
    image: python:3.11-slim
    ports:
      - "5000:5000"
    environment:
      - MLFLOW_BACKEND_STORE_URI=postgresql://postgres:postgres@db:5432/mlflow
      - MLFLOW_DEFAULT_ARTIFACT_ROOT=/mlflow/artifacts
    volumes:
      - mlflow_artifacts:/mlflow/artifacts
      - ./mlruns:/mlflow/mlruns
    depends_on:
      - db
    restart: unless-stopped
    networks:
      - energy-net
    command: >
      bash -c "
        pip install mlflow psycopg2-binary &&
        mlflow server 
        --backend-store-uri postgresql://postgres:postgres@db:5432/mlflow
        --default-artifact-root /mlflow/artifacts
        --host 0.0.0.0
        --port 5000
      "
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:5000/health"]
      interval: 30s
      timeout: 10s
      retries: 3

  # Jupyter Notebook for Analysis
  jupyter:
    build:
      context: .
      dockerfile: Dockerfile
      target: development
    ports:
      - "8888:8888"
    environment:
      - JUPYTER_ENABLE_LAB=yes
    volumes:
      - ./notebooks:/app/notebooks
      - ./app:/app/app
      - ./data:/app/data
      - ./mlruns:/app/mlruns
    restart: unless-stopped
    networks:
      - energy-net
    command: >
      bash -c "
        .venv/bin/pip install jupyterlab &&
        .venv/bin/jupyter lab --ip=0.0.0.0 --port=8888 --no-browser --allow-root --NotebookApp.token='' --NotebookApp.password=''
      "

  # Background Jobs Scheduler
  scheduler:
    build:
      context: .
      dockerfile: Dockerfile
      target: development
    environment:
      - DATABASE_URL=postgresql://postgres:postgres@db:5432/energy_forecasting
      - MLFLOW_TRACKING_URI=http://mlflow:5000
      - REDIS_URL=redis://redis:6379/0
    volumes:
      - ./jobs:/app/jobs
      - ./app:/app/app
      - ./data:/app/data
      - ./mlruns:/app/mlruns
    depends_on:
      - db
      - redis
      - mlflow
    restart: unless-stopped
    networks:
      - energy-net
    command: >
      bash -c "
        .venv/bin/pip install schedule &&
        python -c '
        import schedule
        import time
        import subprocess
        import logging

        logging.basicConfig(level=logging.INFO)
        logger = logging.getLogger(__name__)

        def run_fetch_data():
            logger.info(\"Running data fetch job\")
            subprocess.run([\"python\", \"jobs/fetch_data.py\"])

        def run_retrain():
            logger.info(\"Running model retraining\")
            subprocess.run([\"python\", \"jobs/retrain.py\"])

        def run_backtest():
            logger.info(\"Running backtest\")
            subprocess.run([\"python\", \"jobs/backtest.py\"])

        # Schedule jobs
        schedule.every(6).hours.do(run_fetch_data)
        schedule.every().day.at(\"02:00\").do(run_retrain)
        schedule.every().week.do(run_backtest)

        logger.info(\"Scheduler started\")
        while True:
            schedule.run_pending()
            time.sleep(60)
        '
      "

  # Monitoring with Prometheus
  prometheus:
    image: prom/prometheus:latest
    ports:
      - "9090:9090"
    volumes:
      - ./monitoring/prometheus.yml:/etc/prometheus/prometheus.yml
      - prometheus_data:/prometheus
    restart: unless-stopped
    networks:
      - energy-net
    command:
      - '--config.file=/etc/prometheus/prometheus.yml'
      - '--storage.tsdb.path=/prometheus'
      - '--web.console.libraries=/etc/prometheus/console_libraries'
      - '--web.console.templates=/etc/prometheus/consoles'
      - '--storage.tsdb.retention.time=200h'
      - '--web.enable-lifecycle'

  # Grafana for Visualization
  grafana:
    image: grafana/grafana:latest
    ports:
      - "3000:3000"
    environment:
      - GF_SECURITY_ADMIN_PASSWORD=admin
    volumes:
      - grafana_data:/var/lib/grafana
      - ./monitoring/grafana:/etc/grafana/provisioning
    restart: unless-stopped
    networks:
      - energy-net
    depends_on:
      - prometheus

volumes:
  postgres_data:
  redis_data:
  mlflow_artifacts:
  prometheus_data:
  grafana_data:

networks:
  energy-net:
    driver: bridge
